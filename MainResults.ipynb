{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pommerman - results\n",
    "## By: Peter Ebert Christensen (s153758), Johan Bloch Madsen(s152991) and Mads Okholm BjÃ¸rn (s153413)\n",
    "\n",
    "Pommerman can be installed by following the instructions at the following link (not required): https://github.com/MultiAgentLearning/playground\n",
    "\n",
    "Docker can be installed by following the instructions at the following link (not required):\n",
    "https://docs.docker.com/install/\n",
    "\n",
    "\n",
    "\n",
    "Please fill in the setting below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous created results\n",
    "# Set to False if:\n",
    "#     * You have installed Pommerman, and\n",
    "#     * You have installed docker, and\n",
    "#     * You want to wait for the games to be replayed\n",
    "#     * Don't worry we will not train again, we will just load a pre-trained network\n",
    "loadPrevious = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import error NSDE! You will not be able to render --> Cannot connect to \"None\"\n"
     ]
    }
   ],
   "source": [
    "if not loadPrevious:\n",
    "    # will print:\n",
    "    # 'Import error NSDE! You will not be able to render --> Cannot connect to \"None\"' \n",
    "    # if run on headless server, no need to worry\n",
    "    import pommerman\n",
    "    from pommerman import agents\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import time\n",
    "\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import datetime\n",
    "\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython import display\n",
    "\n",
    "# Our own files\n",
    "from convertInputMapToTrainingLayers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the network\n",
    "\n",
    "First our main network, an ActorCritic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0., std=0.1)\n",
    "        nn.init.constant_(m.bias, 0.1)\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size, std=0.0):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        \n",
    "        self.critic_con = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.critic_linear = nn.Sequential(\n",
    "            nn.Linear(3*3*64, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        self.actor_con = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor_linear = nn.Sequential(\n",
    "            nn.Linear(3*3*64, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, num_outputs)\n",
    "        )\n",
    "        \n",
    "        self.log_std = nn.Parameter(torch.ones(num_outputs) * std)\n",
    "        \n",
    "        self.apply(init_weights)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        value = self.critic_con(x)\n",
    "        value = self.critic_linear(value.view(-1, 3*3*64))\n",
    "        \n",
    "        mu    = self.actor_con(x)\n",
    "        mu    = self.actor_linear(mu.view(-1, 3*3*64))\n",
    "        \n",
    "        std1  = self.log_std.exp()\n",
    "        std   = std1.expand_as(mu)\n",
    "        dist  = Normal(mu, std)\n",
    "        return dist, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then our random network for random network distillation (This network is not used in this notebook, as it is not needed to train):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RND(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs, hidden_size):\n",
    "        super(RND, self).__init__()\n",
    "        self.Feature = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=7,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.Conv2d(in_channels=64,\n",
    "                      out_channels=64, \n",
    "                      kernel_size=3, \n",
    "                      padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.Feature_linear = nn.Sequential(\n",
    "            nn.Linear(3*3*64, hidden_size),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        value = self.Feature(x)\n",
    "        value = self.Feature_linear(value.view(-1, 3*3*64))\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs       = 324\n",
    "num_outputs      = 6\n",
    "hidden_size      = 1024\n",
    "lr               = 1e-6\n",
    "lr_RND           = 1e-3\n",
    "mini_batch_size  = 5\n",
    "ppo_epochs       = 4\n",
    "max_frames       = 1500000\n",
    "frame_idx        = 0\n",
    "game_idx         = 0\n",
    "device           = \"cpu\" # Hard-coded since we have a GPU, but does not want to use\n",
    "clip_param       = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Networks (loading pre-trained network)\n",
    "\n",
    "We trained 4 identical networks, with random start weights. After 1 500 000 frames of training each we took the two best networks, and trained a copy of them with a renewed reward function, until 3 200 000 Frames each.\n",
    "\n",
    "The win rate is based on test play against 3 simple agents in FFA mode:\n",
    "\n",
    "![01](images/TrainAI01.png \"AI 01\")\n",
    "![02](images/TrainAI02.png \"AI 02\")\n",
    "![03](images/TrainAI03.png \"AI 03\")\n",
    "![04](images/TrainAI04.png \"AI 04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mads12bjoern/anaconda3/lib/python3.7/site-packages/torch/serialization.py:391: UserWarning: Couldn't retrieve source code for container of type ActorCritic. It won't be checked for correctness upon loading.\n",
      "  \"type \" + container_type.__name__ + \". It won't be checked \"\n"
     ]
    }
   ],
   "source": [
    "model = ActorCritic(num_inputs, num_outputs, hidden_size).to(device)\n",
    "model = torch.load(\"models/newAI02_from_oldAI04.pth\", map_location=device)\n",
    "# Correctness warning may appear, do not worry, this is normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "\n",
    "Pre-computed win rates:\n",
    "\n",
    "| # Game | # Network controlled AI | # Simple Agents | # Random Agents | Win Rate |\n",
    "|--------|-------------------------|-----------------|-----------------|----------|\n",
    "|    500 |                       1 |               0 |               3 |    100 % |\n",
    "|    300 |                       1 |               1 |               2 |     85 % |\n",
    "|    300 |                       1 |               2 |               1 |     64 % |\n",
    "|    300 |                       1 |               3 |               0 |     53 % |\n",
    "|    300 |                       2 |               1 |               1 |     96 % |\n",
    "|    300 |                       2 |               2 |               0 |     81 % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment for calculating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played 2 games, with a win rate of 0.0%  (with 1 games not ending in a tie)\n",
      "Played 3 games, with a win rate of 0.0%  (with 2 games not ending in a tie)\n",
      "Played 4 games, with a win rate of 0.0%  (with 3 games not ending in a tie)\n",
      "Played 5 games, with a win rate of 25.0%  (with 4 games not ending in a tie)\n",
      "Played 6 games, with a win rate of 25.0%  (with 4 games not ending in a tie)\n",
      "Played 7 games, with a win rate of 25.0%  (with 4 games not ending in a tie)\n",
      "Played 8 games, with a win rate of 20.0%  (with 5 games not ending in a tie)\n",
      "Played 9 games, with a win rate of 16.666666666666664%  (with 6 games not ending in a tie)\n",
      "Played 10 games, with a win rate of 16.666666666666664%  (with 6 games not ending in a tie)\n",
      "Played 11 games, with a win rate of 28.57142857142857%  (with 7 games not ending in a tie)\n",
      "Played 12 games, with a win rate of 28.57142857142857%  (with 7 games not ending in a tie)\n",
      "Played 13 games, with a win rate of 28.57142857142857%  (with 7 games not ending in a tie)\n",
      "Played 14 games, with a win rate of 25.0%  (with 8 games not ending in a tie)\n",
      "Played 15 games, with a win rate of 33.33333333333333%  (with 9 games not ending in a tie)\n",
      "Played 16 games, with a win rate of 40.0%  (with 10 games not ending in a tie)\n",
      "Played 17 games, with a win rate of 45.45454545454545%  (with 11 games not ending in a tie)\n",
      "Played 18 games, with a win rate of 41.66666666666667%  (with 12 games not ending in a tie)\n",
      "Played 19 games, with a win rate of 38.46153846153847%  (with 13 games not ending in a tie)\n",
      "Played 20 games, with a win rate of 42.857142857142854%  (with 14 games not ending in a tie)\n",
      "Played 21 games, with a win rate of 46.666666666666664%  (with 15 games not ending in a tie)\n",
      "Played 22 games, with a win rate of 46.666666666666664%  (with 15 games not ending in a tie)\n",
      "Played 23 games, with a win rate of 46.666666666666664%  (with 15 games not ending in a tie)\n",
      "Played 24 games, with a win rate of 50.0%  (with 16 games not ending in a tie)\n",
      "Played 25 games, with a win rate of 47.05882352941176%  (with 17 games not ending in a tie)\n",
      "Played 26 games, with a win rate of 50.0%  (with 18 games not ending in a tie)\n",
      "Played 27 games, with a win rate of 47.368421052631575%  (with 19 games not ending in a tie)\n",
      "Played 28 games, with a win rate of 47.368421052631575%  (with 19 games not ending in a tie)\n",
      "Played 29 games, with a win rate of 50.0%  (with 20 games not ending in a tie)\n",
      "Played 30 games, with a win rate of 50.0%  (with 20 games not ending in a tie)\n",
      "Played 31 games, with a win rate of 52.38095238095239%  (with 21 games not ending in a tie)\n",
      "Played 32 games, with a win rate of 54.54545454545454%  (with 22 games not ending in a tie)\n",
      "Played 33 games, with a win rate of 52.17391304347826%  (with 23 games not ending in a tie)\n",
      "Played 34 games, with a win rate of 54.166666666666664%  (with 24 games not ending in a tie)\n",
      "Played 35 games, with a win rate of 52.0%  (with 25 games not ending in a tie)\n",
      "Played 36 games, with a win rate of 50.0%  (with 26 games not ending in a tie)\n",
      "Played 37 games, with a win rate of 51.85185185185185%  (with 27 games not ending in a tie)\n",
      "Played 38 games, with a win rate of 50.0%  (with 28 games not ending in a tie)\n",
      "Played 39 games, with a win rate of 50.0%  (with 28 games not ending in a tie)\n",
      "Played 40 games, with a win rate of 50.0%  (with 28 games not ending in a tie)\n",
      "Played 41 games, with a win rate of 51.724137931034484%  (with 29 games not ending in a tie)\n",
      "Played 42 games, with a win rate of 53.333333333333336%  (with 30 games not ending in a tie)\n",
      "Played 43 games, with a win rate of 54.83870967741935%  (with 31 games not ending in a tie)\n",
      "Played 44 games, with a win rate of 56.25%  (with 32 games not ending in a tie)\n",
      "Played 45 games, with a win rate of 57.57575757575758%  (with 33 games not ending in a tie)\n",
      "Played 46 games, with a win rate of 57.57575757575758%  (with 33 games not ending in a tie)\n",
      "Played 47 games, with a win rate of 58.82352941176471%  (with 34 games not ending in a tie)\n",
      "Played 48 games, with a win rate of 60.0%  (with 35 games not ending in a tie)\n",
      "Played 49 games, with a win rate of 58.333333333333336%  (with 36 games not ending in a tie)\n",
      "Played 50 games, with a win rate of 58.333333333333336%  (with 36 games not ending in a tie)\n",
      "Played 51 games, with a win rate of 56.75675675675676%  (with 37 games not ending in a tie)\n",
      "Played 52 games, with a win rate of 56.75675675675676%  (with 37 games not ending in a tie)\n",
      "Played 53 games, with a win rate of 56.75675675675676%  (with 37 games not ending in a tie)\n",
      "Played 54 games, with a win rate of 55.26315789473685%  (with 38 games not ending in a tie)\n"
     ]
    }
   ],
   "source": [
    "if not loadPrevious:\n",
    "    def playGame():\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            stateOrginal = state\n",
    "            state = torch.FloatTensor(stateToTorch(state)).to(device)\n",
    "            dist, _ = model(state)\n",
    "            actionsList = env.act(stateOrginal)\n",
    "            state, reward, done, info = env.step([dist.mean.cpu().data.numpy()[0].argmax()] + actionsList[1:])\n",
    "        if \"winners\" in info:\n",
    "            if 0 in info[\"winners\"]:\n",
    "                return \"Won\"\n",
    "            else:\n",
    "                return \"Lost\"\n",
    "        else:\n",
    "            return \"Tie\"\n",
    "    \n",
    "    # Create a set of agents (exactly four)\n",
    "    agent_list = [\n",
    "        agents.RandomAgent(), # Does not matter, we control this agent\n",
    "        agents.SimpleAgent(), # Replace with RandomAgent for easier games\n",
    "        agents.SimpleAgent(), # Replace with RandomAgent for easier games\n",
    "        agents.SimpleAgent(), # Replace with RandomAgent for easier games\n",
    "    ]\n",
    "    # Make the \"Free-For-All\" environment using the agent list\n",
    "    env = pommerman.make('PommeFFACompetitionFast-v0', agent_list)\n",
    "    \n",
    "    gamesPalyed    = 0\n",
    "    gamesWon       = 0\n",
    "    gamesThatCount = 0 # Games that end in a tie is replayed, and therefore \n",
    "    while True:\n",
    "        result = playGame()\n",
    "        gamesPalyed += 1\n",
    "        if result == \"Won\":\n",
    "            gamesWon += 1\n",
    "        if result != \"Tie\":\n",
    "            gamesThatCount += 1\n",
    "        if gamesThatCount != 0:\n",
    "            print(\"Played \" + str(gamesPalyed) + \" games, with a win rate of \" + str(gamesWon/float(gamesThatCount) * 100.0) + \"%  (with \" + str(gamesThatCount) + \" games not ending in a tie)\")\n",
    "        \n",
    "else:\n",
    "    print(\"Disable loadPrevious\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please note this make take upwards of 300 games to stabilize"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
